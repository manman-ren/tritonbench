{"hash": "25e63e8636682ef5433c774ad2b73526bd99c5db00fb07c6ac48e1c7d208ab2d", "target": {"backend": "cuda", "arch": 100, "warp_size": 32}, "num_warps": 20, "num_ctas": 1, "num_stages": 0, "num_buffers_warp_spec": 2, "num_consumer_groups": 2, "reg_dec_producer": 24, "reg_inc_consumer": 240, "maxnreg": null, "cluster_dims": [1, 1, 1], "ptx_version": null, "enable_fp_fusion": true, "launch_cooperative_grid": false, "launch_pdl": false, "supported_fp8_dtypes": ["fp8e4b15", "fp8e4nv", "fp8e5"], "deprecated_fp8_dtypes": ["fp8e4b15"], "default_dot_input_precision": "tf32", "allowed_dot_input_precisions": ["tf32", "tf32x3", "ieee"], "max_num_imprecise_acc_default": 0, "extern_libs": [["libdevice", "/home/mren/MetaMain2/triton/python/triton/backends/nvidia/lib/libdevice.10.bc"]], "debug": false, "backend_name": "cuda", "sanitize_overflow": true, "arch": "sm100", "triton_version": "3.3.0", "tensordesc_meta": [], "shared": 230472, "tmem_size": 256, "global_scratch_size": 768, "global_scratch_align": 128, "name": "_attn_fwd_tma_ws_persistent"}