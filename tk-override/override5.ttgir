#blocked = #ttg.blocked<{sizePerThread = [1, 32], threadsPerWarp = [16, 2], warpsPerCTA = [4, 2], order = [0, 1]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>
#loc = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0)
#loc1 = loc(unknown)
#loc2 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":280:29)
#loc3 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":992:12)
#loc4 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1697:12)
#loc5 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":281:29)
#loc6 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":344:33)
#loc7 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":302:27)
#loc8 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":345:33)
#loc9 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":325:27)
#loc49 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":289:53)
#loc56 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":292:31)
#loc87 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":312:53)
#loc94 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":315:31)
#shared = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0]}>
#shared1 = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = false, elementBitWidth = 16}>
#shared2 = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = true, elementBitWidth = 16}>
#smem = #ttg.shared_memory
#tmem = #ttng.tensor_memory_encoding<blockM = 64, blockN = 128, unpacked = true>
#loc126 = loc(callsite(#loc3 at #loc4))
#loc178 = loc(callsite(#loc2 at #loc126))
#loc179 = loc(callsite(#loc5 at #loc126))
#loc180 = loc(callsite(#loc6 at #loc126))
#loc181 = loc(callsite(#loc7 at #loc126))
#loc182 = loc(callsite(#loc8 at #loc126))
#loc183 = loc(callsite(#loc9 at #loc126))
#loc187 = loc(callsite(#loc49 at #loc126))
#loc192 = loc(callsite(#loc56 at #loc126))
#loc200 = loc(callsite(#loc87 at #loc126))
#loc207 = loc(callsite(#loc94 at #loc126))
#loc224 = loc(callsite(#loc1 at #loc187))
#loc226 = loc(callsite(#loc1 at #loc192))
#loc228 = loc(callsite(#loc1 at #loc200))
#loc230 = loc(callsite(#loc1 at #loc207))
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 8 : i32, ttg.target = "cuda:100", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_attn_fwd_tma_ws_persistent(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg3: f32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg4: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg5: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg6: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg7: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg8: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg9: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg10: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg11: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg12: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg13: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg14: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg15: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg16: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg17: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg18: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg19: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg20: i32 {tt.divisibility = 16 : i32} loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0)) attributes {noinline = false} {
    %c256_i64 = arith.constant 256 : i64 loc(#loc1)
    %c64_i32 = arith.constant 64 : i32 loc(#loc1)
    %c0_i64 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 0 : i64 loc(#loc1)
    %cst = arith.constant {async_task_id = dense<0> : vector<1xi32>} dense<1.000000e+00> : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
    %cst_0 = arith.constant {async_task_id = dense<0> : vector<1xi32>} dense<0xFF800000> : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
    %cst_1 = arith.constant {async_task_id = dense<0> : vector<1xi32>} dense<0.000000e+00> : tensor<64x128xf32, #blocked> loc(#loc1)
    %cst_2 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 1.44269502 : f32 loc(#loc1)
    %c1_i64 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 1 : i64 loc(#loc1)
    %c128_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 128 : i32 loc(#loc1)
    %c127_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 127 : i32 loc(#loc1)
    %true = arith.constant {async_task_id = dense<0> : vector<1xi32>} true loc(#loc1)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %0 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %1 = ttg.memdesc_subview %0[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %1, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %2 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %3 = ttg.memdesc_subview %2[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %3, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %4 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %5 = ttg.memdesc_subview %4[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %5, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %6 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %7 = ttg.memdesc_subview %6[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %7, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %8 = ttg.local_alloc : () -> !ttg.memdesc<2xi64, #shared, #smem, mutable> loc(#loc)
    %9 = ttg.memdesc_subview %8[%c0_i32] : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %9, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %10 = ttg.memdesc_subview %8[%c1_i32] : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %10, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %11 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %12 = ttg.memdesc_subview %11[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %12, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %13 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %14 = ttg.memdesc_subview %13[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %14, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %15 = ttg.local_alloc : () -> !ttg.memdesc<2xi64, #shared, #smem, mutable> loc(#loc)
    %16 = ttg.memdesc_subview %15[%c0_i32] : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %16, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %17 = ttg.memdesc_subview %15[%c1_i32] : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %17, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %18 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %19 = ttg.memdesc_subview %18[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %19, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %20 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %21 = ttg.memdesc_subview %20[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %21, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %22 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %23 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %24 = ttg.memdesc_subview %22[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %24, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %25 = ttg.memdesc_subview %23[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %25, 32 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %26 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %27 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %28 = ttg.memdesc_subview %26[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %28, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %29 = ttg.memdesc_subview %27[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %29, 32 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %30 = ttg.local_alloc : () -> !ttg.memdesc<2xi64, #shared, #smem, mutable> loc(#loc)
    %31 = ttg.local_alloc : () -> !ttg.memdesc<2xi64, #shared, #smem, mutable> loc(#loc)
    %32 = ttg.memdesc_subview %30[%c0_i32] : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %32, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %33 = ttg.memdesc_subview %31[%c0_i32] : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %33, 32 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %34 = ttg.memdesc_subview %30[%c1_i32] : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %34, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %35 = ttg.memdesc_subview %31[%c1_i32] : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %35, 32 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %36 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %37 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %38 = ttg.memdesc_subview %36[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %38, 32 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %39 = ttg.memdesc_subview %37[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %39, 256 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %40 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %41 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %42 = ttg.memdesc_subview %40[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %42, 32 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %43 = ttg.memdesc_subview %41[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %43, 256 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %44 = ttg.local_alloc : () -> !ttg.memdesc<2xi64, #shared, #smem, mutable> loc(#loc)
    %45 = ttg.local_alloc : () -> !ttg.memdesc<2xi64, #shared, #smem, mutable> loc(#loc)
    %46 = ttg.memdesc_subview %44[%c0_i32] : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %46, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %47 = ttg.memdesc_subview %45[%c0_i32] : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %47, 32 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %48 = ttg.memdesc_subview %44[%c1_i32] : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %48, 1 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %49 = ttg.memdesc_subview %45[%c1_i32] : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %49, 32 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %50 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %51 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %52 = ttg.memdesc_subview %50[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %52, 32 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %53 = ttg.memdesc_subview %51[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %53, 256 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %54 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %55 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %56 = ttg.memdesc_subview %54[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %56, 256 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %57 = ttg.memdesc_subview %55[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %57, 32 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %58 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %59 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %60 = ttg.memdesc_subview %58[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %60, 32 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %61 = ttg.memdesc_subview %59[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %61, 256 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %62 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %63 = ttg.local_alloc : () -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %64 = ttg.memdesc_subview %62[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %64, 256 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    %65 = ttg.memdesc_subview %63[%c0_i32] : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    ttng.init_barrier %65, 32 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %66 = ttg.local_alloc : () -> !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(#loc)
    %67 = ttg.local_alloc : () -> !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(#loc)
    %68 = ttg.local_alloc : () -> !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> loc(#loc)
    %result = ttng.tmem_alloc : () -> !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc178)
    %result_3 = ttng.tmem_alloc : () -> !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc179)
    %69 = ttg.local_alloc : () -> !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> loc(#loc)
    %result_4 = ttng.tmem_alloc : () -> !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc180)
    %70 = ttg.local_alloc : () -> !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(#loc181)
    %result_5 = ttng.tmem_alloc : () -> !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc182)
    %71 = ttg.local_alloc : () -> !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(#loc183)
    ttg.warp_specialize(%arg20, %arg18, %arg19, %arg0, %arg5, %arg6, %arg7, %arg3, %arg8, %arg17, %18, %67, %result_3, %result_5, %11, %2, %71, %0, %arg4, %20, %66, %result, %15, %68, %13, %result_4, %70, %8, %69, %4, %6, %arg1, %arg11, %arg2, %arg14, %23, %27, %31, %37, %41, %45, %51, %54, %59, %62)
    default {
      %72 = arith.addi %arg20, %c127_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc127)
      %73 = arith.divsi %72, %c128_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc128)
      %74 = tt.get_program_id x {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc13)
      %75 = tt.get_num_programs x {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc14)
      %76 = arith.muli %73, %arg18 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc15)
      %77 = arith.muli %76, %arg19 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc16)
      %78 = arith.divsi %77, %75 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc17)
      %79 = arith.remsi %77, %75 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc18)
      %80 = arith.cmpi slt, %74, %79 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc19)
      %81 = scf.if %80 -> (i32) {
        %95 = arith.addi %78, %c1_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc21)
        scf.yield {async_task_id = dense<0> : vector<1xi32>} %95 : i32 loc(#loc21)
      } else {
        scf.yield {async_task_id = dense<0> : vector<1xi32>} %78 : i32 loc(#loc1)
      } {async_task_id = dense<0> : vector<1xi32>} loc(#loc20)
      %82 = arith.muli %arg18, %arg19 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
      %83 = arith.muli %82, %arg20 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc23)
      %84 = ttg.global_scratch_alloc {alignment = 128 : i32, nbytes = 128 : i32} : !tt.ptr<i8> loc(#loc24)
      tt.experimental_tensormap_create %84, %arg0, [%c64_i32, %c64_i32], [%c128_i32, %83], [%c256_i64], [%c1_i32, %c1_i32] {elem_type = 10 : i32, fill_mode = 0 : i32, interleave_layout = 0 : i32, swizzle_mode = 3 : i32} : (!tt.ptr<i8>, !tt.ptr<bf16>, i32, i32, i32, i32, i64, i32, i32) -> () loc(#loc24)
      tt.experimental_tensormap_fenceproxy_acquire %84 : !tt.ptr<i8> loc(#loc24)
      %85 = ttg.global_scratch_alloc {alignment = 128 : i32, nbytes = 128 : i32} : !tt.ptr<i8> loc(#loc25)
      tt.experimental_tensormap_create %85, %arg5, [%c64_i32, %c64_i32], [%c128_i32, %83], [%c256_i64], [%c1_i32, %c1_i32] {elem_type = 10 : i32, fill_mode = 0 : i32, interleave_layout = 0 : i32, swizzle_mode = 3 : i32} : (!tt.ptr<i8>, !tt.ptr<bf16>, i32, i32, i32, i32, i64, i32, i32) -> () loc(#loc25)
      tt.experimental_tensormap_fenceproxy_acquire %85 : !tt.ptr<i8> loc(#loc25)
      %86 = arith.extsi %arg6 {async_task_id = dense<0> : vector<1xi32>} : i32 to i64 loc(#loc129)
      %87 = arith.extsi %arg7 {async_task_id = dense<0> : vector<1xi32>} : i32 to i64 loc(#loc130)
      %88 = tt.make_range {async_task_id = dense<0> : vector<1xi32>, end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked1> loc(#loc131)
      %89 = arith.mulf %arg3, %cst_2 {async_task_id = dense<0> : vector<1xi32>} : f32 loc(#loc132)
      %90 = arith.extsi %arg8 {async_task_id = dense<0> : vector<1xi32>} : i32 to i64 loc(#loc133)
      %91 = tt.splat %89 {async_task_id = dense<0> : vector<1xi32>} : f32 -> tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc184)
      %92 = tt.splat %89 {async_task_id = dense<0> : vector<1xi32>} : f32 -> tensor<64x128xf32, #blocked> loc(#loc185)
      %93 = arith.extsi %arg17 {async_task_id = dense<0> : vector<1xi32>} : i32 to i64 loc(#loc134)
      %94:3 = scf.for %arg21 = %c0_i32 to %81 step %c1_i32 iter_args(%arg22 = %74, %arg23 = %c0_i64, %arg24 = %c0_i64) -> (i32, i64, i64)  : i32 {
        %95 = arith.remsi %arg22, %73 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc35)
        %96 = arith.divsi %arg22, %73 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc36)
        %97 = arith.divsi %96, %arg19 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc135)
        %98 = arith.remsi %96, %arg19 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc136)
        %99 = arith.extsi %97 {async_task_id = dense<0> : vector<1xi32>} : i32 to i64 loc(#loc137)
        %100 = arith.muli %99, %86 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc129)
        %101 = arith.extsi %98 {async_task_id = dense<0> : vector<1xi32>} : i32 to i64 loc(#loc138)
        %102 = arith.muli %101, %87 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc130)
        %103 = arith.addi %100, %102 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc139)
        %104 = arith.muli %95, %c128_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc140)
        %105 = tt.splat %104 {async_task_id = dense<0> : vector<1xi32>} : i32 -> tensor<64xi32, #blocked1> loc(#loc141)
        %106 = arith.addi %105, %88 {async_task_id = dense<0> : vector<1xi32>} : tensor<64xi32, #blocked1> loc(#loc141)
        %107 = arith.divsi %103, %90 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc133)
        %108 = arith.extsi %104 {async_task_id = dense<0> : vector<1xi32>} : i32 to i64 loc(#loc142)
        %109 = arith.addi %107, %108 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc142)
        %110 = arith.trunci %109 {async_task_id = dense<0> : vector<1xi32>} : i64 to i32 loc(#loc143)
        %111 = arith.andi %arg23, %c1_i64 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc144)
        %112 = arith.trunci %111 {async_task_id = dense<0> : vector<1xi32>} : i64 to i1 loc(#loc144)
        %113 = ttg.memdesc_subview %23[%c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
        %114 = arith.xori %112, %true : i1 loc(#loc144)
        %115 = arith.extui %114 : i1 to i32 loc(#loc144)
        ttng.wait_barrier %113, %115 {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc144)
        %116 = ttg.memdesc_subview %20[%c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
        ttng.barrier_expect %116, 16384 {async_task_id = dense<0> : vector<1xi32>}, %true : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
        %117 = ttg.memdesc_subview %66[%c0_i32, %c0_i32, %c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable> loc(#loc)
        // tt.print "softmax0: before tma q0 " {hex = false, isSigned = array<i32: 2>} : %arg21 : i32
        ttng.async_tma_copy_global_to_local %84[%110, %c0_i32] %117, %116, %true {async_task_id = dense<0> : vector<1xi32>} : !tt.ptr<i8>, !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable> loc(#loc)
        %118 = ttg.memdesc_subview %result[%c0_i32, %c0_i32, %c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> loc(#loc178)
        %119 = ttg.memdesc_subview %result_4[%c0_i32, %c0_i32, %c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> loc(#loc180)
        ttng.tmem_store %cst_1, %119, %true {async_task_id = dense<0> : vector<1xi32>} : tensor<64x128xf32, #blocked> -> !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> loc(#loc180)
        %120:3 = scf.for %arg25 = %c0_i32 to %arg20 step %c128_i32 iter_args(%arg26 = %cst, %arg27 = %cst_0, %arg28 = %arg24) -> (tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, i64)  : i32 {
          %138 = arith.andi %arg28, %c1_i64 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc178)
          %139 = arith.trunci %138 {async_task_id = dense<0> : vector<1xi32>} : i64 to i1 loc(#loc178)
          %140 = ttg.memdesc_subview %13[%c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %141 = arith.extsi %139 {async_task_id = dense<0> : vector<1xi32>} : i1 to i32 loc(#loc178)
          ttng.wait_barrier %140, %141 {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc178)
          %result_7 = ttng.tmem_load %118 {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> -> tensor<64x128xf32, #blocked> loc(#loc178)
          %142 = ttg.memdesc_subview %37[%c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          ttng.mbarrier_arrive %142 {async_task_id = dense<0> : vector<1xi32>, operandSegmentSizes = array<i32: 1, 0, 0>, trackAsyncOp = false} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc178)
          // tt.print "softmax0: after softmax0 get qk0 " {hex = false, isSigned = array<i32: 2>} : %arg25 : i32
          %143 = "tt.reduce"(%result_7) <{axis = 1 : i32}> ({
          ^bb0(%arg29: f32 loc(callsite(#loc1 at #loc187)), %arg30: f32 loc(callsite(#loc1 at #loc187))):
            %168 = arith.maxnumf %arg29, %arg30 {async_task_id = dense<0> : vector<1xi32>} : f32 loc(#loc231)
            tt.reduce.return %168 {async_task_id = dense<0> : vector<1xi32>} : f32 loc(#loc223)
          }) {async_task_id = dense<0> : vector<1xi32>} : (tensor<64x128xf32, #blocked>) -> tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc223)
          // tt.print "softmax0: after softmax0 first reduction " {hex = false, isSigned = array<i32: 2>} : %arg25 : i32
          %144 = arith.mulf %143, %91 {async_task_id = dense<0> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc184)
          %145 = arith.maxnumf %arg27, %144 {async_task_id = dense<0> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc188)
          %146 = arith.mulf %result_7, %92 {async_task_id = dense<0> : vector<1xi32>} : tensor<64x128xf32, #blocked> loc(#loc185)
          %147 = tt.expand_dims %145 {async_task_id = dense<0> : vector<1xi32>, axis = 1 : i32} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64x1xf32, #blocked> loc(#loc189)
          %148 = tt.broadcast %147 {async_task_id = dense<0> : vector<1xi32>} : tensor<64x1xf32, #blocked> -> tensor<64x128xf32, #blocked> loc(#loc190)
          %149 = arith.subf %146, %148 {async_task_id = dense<0> : vector<1xi32>} : tensor<64x128xf32, #blocked> loc(#loc190)
          %150 = math.exp2 %149 {async_task_id = dense<0> : vector<1xi32>} : tensor<64x128xf32, #blocked> loc(#loc191)
          %151 = "tt.reduce"(%150) <{axis = 1 : i32}> ({
          ^bb0(%arg29: f32 loc(callsite(#loc1 at #loc192)), %arg30: f32 loc(callsite(#loc1 at #loc192))):
            %168 = arith.addf %arg29, %arg30 {async_task_id = dense<0> : vector<1xi32>} : f32 loc(#loc232)
            tt.reduce.return %168 {async_task_id = dense<0> : vector<1xi32>} : f32 loc(#loc225)
          }) {async_task_id = dense<0> : vector<1xi32>} : (tensor<64x128xf32, #blocked>) -> tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc225)
          // tt.print "softmax0: after softmax0 second reduction " {hex = false, isSigned = array<i32: 2>} : %arg25 : i32
          %152 = arith.subf %arg27, %145 {async_task_id = dense<0> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc193)
          %153 = math.exp2 %152 {async_task_id = dense<0> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc194)
          %154 = arith.mulf %arg26, %153 {async_task_id = dense<0> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc195)
          %155 = arith.addf %154, %151 {async_task_id = dense<0> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc196)
          %156 = tt.expand_dims %153 {async_task_id = dense<0> : vector<1xi32>, axis = 1 : i32} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64x1xf32, #blocked> loc(#loc197)
          %157 = tt.broadcast %156 {async_task_id = dense<0> : vector<1xi32>} : tensor<64x1xf32, #blocked> -> tensor<64x128xf32, #blocked> loc(#loc198)
          %158 = ttg.memdesc_subview %6[%c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          // CHANGE from %141
          %164 = arith.xori %139, %true {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc181)
          %165 = arith.extsi %164 {async_task_id = dense<0> : vector<1xi32>} : i1 to i32 loc(#loc181)
          // tt.print "softmax0: before softmax0 wait_barrier acc0 " {hex = false, isSigned = array<i32: 2>} : %arg25 : i32
          ttng.wait_barrier %158, %165 {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc180)
          // tt.print "softmax0: before softmax0 get acc0 " {hex = false, isSigned = array<i32: 2>} : %arg25 : i32
          %result_8 = ttng.tmem_load %119 {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> -> tensor<64x128xf32, #blocked> loc(#loc180)
          %159 = ttg.memdesc_subview %51[%c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          ttng.mbarrier_arrive %159 {async_task_id = dense<0> : vector<1xi32>, operandSegmentSizes = array<i32: 1, 0, 0>, trackAsyncOp = false} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc180)
          // tt.print "softmax0: arrive var51 " {hex = false, isSigned = array<i32: 2>} : %arg25 : i32
          %160 = arith.mulf %result_8, %157 {async_task_id = dense<0> : vector<1xi32>} : tensor<64x128xf32, #blocked> loc(#loc198)
          %161 = arith.truncf %150 {async_task_id = dense<0> : vector<1xi32>} : tensor<64x128xf32, #blocked> to tensor<64x128xbf16, #blocked> loc(#loc181)
          %162 = ttg.memdesc_subview %70[%c0_i32, %c0_i32, %c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable, 1x64x128> loc(#loc181)
          %163 = ttg.memdesc_subview %4[%c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          // %164 = arith.xori %139, %true {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc181)
          // %165 = arith.extsi %164 {async_task_id = dense<0> : vector<1xi32>} : i1 to i32 loc(#loc181)
          ttng.wait_barrier %163, %165 {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc181)
          ttg.local_store %161, %162 {async_task_id = dense<0> : vector<1xi32>} : tensor<64x128xbf16, #blocked> -> !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable, 1x64x128> loc(#loc181)
          %166 = ttg.memdesc_subview %54[%c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          ttng.mbarrier_arrive %166, %true {async_task_id = dense<0> : vector<1xi32>, operandSegmentSizes = array<i32: 1, 1, 0>, trackAsyncOp = true} : !ttg.memdesc<1xi64, #shared, #smem, mutable>, i1 loc(#loc181)
          ttng.tmem_store %160, %119, %true {async_task_id = dense<0> : vector<1xi32>} : tensor<64x128xf32, #blocked> -> !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> loc(#loc180)
          %167 = arith.addi %arg28, %c1_i64 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc186)
          scf.yield {async_task_id = dense<0> : vector<1xi32>} %155, %145, %167 : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, i64 loc(#loc199)
        } {async_task_id = dense<0> : vector<1xi32>} loc(#loc186)
        %121 = math.log2 %120#0 {async_task_id = dense<0> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc145)
        %122 = arith.addf %120#1, %121 {async_task_id = dense<0> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc146)
        %123 = tt.expand_dims %120#0 {async_task_id = dense<0> : vector<1xi32>, axis = 1 : i32} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64x1xf32, #blocked> loc(#loc147)
        %124 = tt.broadcast %123 {async_task_id = dense<0> : vector<1xi32>} : tensor<64x1xf32, #blocked> -> tensor<64x128xf32, #blocked> loc(#loc148)
        %result_6 = ttng.tmem_load %119 {async_task_id = dense<0> : vector<1xi32>} : !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> -> tensor<64x128xf32, #blocked> loc(#loc180)
        %125 = arith.divf %result_6, %124 {async_task_id = dense<0> : vector<1xi32>} : tensor<64x128xf32, #blocked> loc(#loc148)
        %126 = arith.muli %96, %arg20 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc149)
        %127 = tt.addptr %arg4, %126 {async_task_id = dense<0> : vector<1xi32>} : !tt.ptr<f32>, i32 loc(#loc150)
        %128 = tt.splat %127 {async_task_id = dense<0> : vector<1xi32>} : !tt.ptr<f32> -> tensor<64x!tt.ptr<f32>, #blocked1> loc(#loc151)
        %129 = tt.addptr %128, %106 {async_task_id = dense<0> : vector<1xi32>} : tensor<64x!tt.ptr<f32>, #blocked1>, tensor<64xi32, #blocked1> loc(#loc151)
        %130 = ttg.convert_layout %122 {async_task_id = dense<0> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64xf32, #blocked1> loc(#loc152)
        tt.store %129, %130 {async_task_id = dense<0> : vector<1xi32>} : tensor<64x!tt.ptr<f32>, #blocked1> loc(#loc152)
        %131 = arith.divsi %103, %93 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc134)
        %132 = arith.addi %131, %108 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc153)
        %133 = arith.trunci %132 {async_task_id = dense<0> : vector<1xi32>} : i64 to i32 loc(#loc154)
        %134 = arith.truncf %125 {async_task_id = dense<0> : vector<1xi32>} : tensor<64x128xf32, #blocked> to tensor<64x128xbf16, #blocked> loc(#loc155)
        %135 = ttg.local_alloc %134 : (tensor<64x128xbf16, #blocked>) -> !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable> loc(#loc156)
        ttng.fence_async_shared {bCluster = false} loc(#loc156)
        ttng.async_tma_copy_local_to_global %85[%133, %c0_i32] %135 : !tt.ptr<i8>, !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable> loc(#loc156)
        ttng.async_tma_store_wait {pendings = 0 : i32} loc(#loc156)
        %136 = arith.addi %arg22, %75 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc77)
        %137 = arith.addi %arg23, %c1_i64 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc34)
        scf.yield {async_task_id = dense<0> : vector<1xi32>} %136, %137, %120#2 : i32, i64, i64 loc(#loc78)
      } {async_task_id = dense<0> : vector<1xi32>} loc(#loc34)
      ttg.warp_yield {async_task_id = dense<0> : vector<1xi32>} loc(#loc)
    }
    partition0(%arg21: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg22: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg23: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg24: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg25: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg26: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg27: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg28: f32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg29: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg30: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg31: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg32: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg33: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc5 at #loc126)), %arg34: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc8 at #loc126)), %arg35: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg36: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg37: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(callsite(#loc9 at #loc126)), %arg38: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg39: !tt.ptr<f32> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg40: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg41: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg42: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc2 at #loc126)), %arg43: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg44: !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg45: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg46: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc6 at #loc126)), %arg47: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(callsite(#loc7 at #loc126)), %arg48: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg49: !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg50: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg51: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg52: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg53: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg54: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg55: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg56: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg57: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg58: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg59: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg60: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg61: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg62: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg63: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg64: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg65: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0)) num_warps(8) {
      %c256_i64_6 = arith.constant 256 : i64 loc(#loc1)
      %c0_i64_7 = arith.constant {async_task_id = dense<1> : vector<1xi32>} 0 : i64 loc(#loc1)
      %true_8 = arith.constant {async_task_id = dense<1> : vector<1xi32>} true loc(#loc1)
      %c127_i32_9 = arith.constant {async_task_id = dense<1> : vector<1xi32>} 127 : i32 loc(#loc1)
      %c1_i32_10 = arith.constant {async_task_id = dense<1> : vector<1xi32>} 1 : i32 loc(#loc1)
      %c128_i32_11 = arith.constant {async_task_id = dense<1> : vector<1xi32>} 128 : i32 loc(#loc1)
      %c1_i64_12 = arith.constant {async_task_id = dense<1> : vector<1xi32>} 1 : i64 loc(#loc1)
      %c0_i32_13 = arith.constant {async_task_id = dense<1> : vector<1xi32>} 0 : i32 loc(#loc1)
      %c64_i32_14 = arith.constant {async_task_id = dense<1> : vector<1xi32>} 64 : i32 loc(#loc1)
      %cst_15 = arith.constant {async_task_id = dense<1> : vector<1xi32>} 1.44269502 : f32 loc(#loc1)
      %c64_i64 = arith.constant {async_task_id = dense<1> : vector<1xi32>} 64 : i64 loc(#loc1)
      %cst_16 = arith.constant {async_task_id = dense<1> : vector<1xi32>} dense<0.000000e+00> : tensor<64x128xf32, #blocked> loc(#loc1)
      %cst_17 = arith.constant {async_task_id = dense<1> : vector<1xi32>} dense<0xFF800000> : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
      %cst_18 = arith.constant {async_task_id = dense<1> : vector<1xi32>} dense<1.000000e+00> : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc1)
      %72 = arith.addi %arg21, %c127_i32_9 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc127)
      %73 = arith.divsi %72, %c128_i32_11 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc128)
      %74 = tt.get_program_id x {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc13)
      %75 = tt.get_num_programs x {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc14)
      %76 = arith.muli %73, %arg22 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc15)
      %77 = arith.muli %76, %arg23 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc16)
      %78 = arith.divsi %77, %75 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc17)
      %79 = arith.remsi %77, %75 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc18)
      %80 = arith.cmpi slt, %74, %79 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc19)
      %81 = scf.if %80 -> (i32) {
        %95 = arith.addi %78, %c1_i32_10 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc21)
        scf.yield {async_task_id = dense<1> : vector<1xi32>} %95 : i32 loc(#loc21)
      } else {
        scf.yield {async_task_id = dense<1> : vector<1xi32>} %78 : i32 loc(#loc1)
      } {async_task_id = dense<1> : vector<1xi32>} loc(#loc20)
      %82 = arith.muli %arg22, %arg23 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
      %83 = arith.muli %82, %arg21 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc23)
      %84 = ttg.global_scratch_alloc {alignment = 128 : i32, nbytes = 128 : i32} : !tt.ptr<i8> loc(#loc24)
      tt.experimental_tensormap_create %84, %arg24, [%c64_i32_14, %c64_i32_14], [%c128_i32_11, %83], [%c256_i64_6], [%c1_i32_10, %c1_i32_10] {elem_type = 10 : i32, fill_mode = 0 : i32, interleave_layout = 0 : i32, swizzle_mode = 3 : i32} : (!tt.ptr<i8>, !tt.ptr<bf16>, i32, i32, i32, i32, i64, i32, i32) -> () loc(#loc24)
      tt.experimental_tensormap_fenceproxy_acquire %84 : !tt.ptr<i8> loc(#loc24)
      %85 = ttg.global_scratch_alloc {alignment = 128 : i32, nbytes = 128 : i32} : !tt.ptr<i8> loc(#loc25)
      tt.experimental_tensormap_create %85, %arg25, [%c64_i32_14, %c64_i32_14], [%c128_i32_11, %83], [%c256_i64_6], [%c1_i32_10, %c1_i32_10] {elem_type = 10 : i32, fill_mode = 0 : i32, interleave_layout = 0 : i32, swizzle_mode = 3 : i32} : (!tt.ptr<i8>, !tt.ptr<bf16>, i32, i32, i32, i32, i64, i32, i32) -> () loc(#loc25)
      tt.experimental_tensormap_fenceproxy_acquire %85 : !tt.ptr<i8> loc(#loc25)
      %86 = arith.extsi %arg26 {async_task_id = dense<1> : vector<1xi32>} : i32 to i64 loc(#loc129)
      %87 = arith.extsi %arg27 {async_task_id = dense<1> : vector<1xi32>} : i32 to i64 loc(#loc130)
      %88 = tt.make_range {async_task_id = dense<1> : vector<1xi32>, end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked1> loc(#loc131)
      %89 = arith.mulf %arg28, %cst_15 {async_task_id = dense<1> : vector<1xi32>} : f32 loc(#loc132)
      %90 = arith.extsi %arg29 {async_task_id = dense<1> : vector<1xi32>} : i32 to i64 loc(#loc157)
      %91 = tt.splat %89 {async_task_id = dense<1> : vector<1xi32>} : f32 -> tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc184)
      %92 = tt.splat %89 {async_task_id = dense<1> : vector<1xi32>} : f32 -> tensor<64x128xf32, #blocked> loc(#loc185)
      %93 = arith.extsi %arg30 {async_task_id = dense<1> : vector<1xi32>} : i32 to i64 loc(#loc158)
      %94:3 = scf.for %arg66 = %c0_i32_13 to %81 step %c1_i32_10 iter_args(%arg67 = %74, %arg68 = %c0_i64_7, %arg69 = %c0_i64_7) -> (i32, i64, i64)  : i32 {
        %95 = arith.remsi %arg67, %73 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc35)
        %96 = arith.divsi %arg67, %73 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc36)
        %97 = arith.divsi %96, %arg23 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc135)
        %98 = arith.remsi %96, %arg23 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc136)
        %99 = arith.extsi %97 {async_task_id = dense<1> : vector<1xi32>} : i32 to i64 loc(#loc137)
        %100 = arith.muli %99, %86 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc129)
        %101 = arith.extsi %98 {async_task_id = dense<1> : vector<1xi32>} : i32 to i64 loc(#loc138)
        %102 = arith.muli %101, %87 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc130)
        %103 = arith.addi %100, %102 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc139)
        %104 = arith.muli %95, %c128_i32_11 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc140)
        %105 = arith.addi %104, %c64_i32_14 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc159)
        %106 = tt.splat %105 {async_task_id = dense<1> : vector<1xi32>} : i32 -> tensor<64xi32, #blocked1> loc(#loc160)
        %107 = arith.addi %106, %88 {async_task_id = dense<1> : vector<1xi32>} : tensor<64xi32, #blocked1> loc(#loc160)
        %108 = arith.divsi %103, %90 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc157)
        %109 = arith.extsi %104 {async_task_id = dense<1> : vector<1xi32>} : i32 to i64 loc(#loc161)
        %110 = arith.addi %108, %109 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc161)
        %111 = arith.addi %110, %c64_i64 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc162)
        %112 = arith.trunci %111 {async_task_id = dense<1> : vector<1xi32>} : i64 to i32 loc(#loc163)
        %113 = arith.andi %arg68, %c1_i64_12 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc164)
        %114 = arith.trunci %113 {async_task_id = dense<1> : vector<1xi32>} : i64 to i1 loc(#loc164)
        %115 = ttg.memdesc_subview %arg57[%c0_i32_13] {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
        %116 = arith.xori %114, %true_8 : i1 loc(#loc164)
        %117 = arith.extui %116 : i1 to i32 loc(#loc164)
        ttng.wait_barrier %115, %117 {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc164)
        %118 = ttg.memdesc_subview %arg31[%c0_i32_13] {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
        ttng.barrier_expect %118, 16384 {async_task_id = dense<1> : vector<1xi32>}, %true_8 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
        %119 = ttg.memdesc_subview %arg32[%c0_i32_13, %c0_i32_13, %c0_i32_13] {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable> loc(#loc)
        // tt.print "softmax1: before tma q1 " {hex = false, isSigned = array<i32: 2>} : %arg66 : i32
        ttng.async_tma_copy_global_to_local %84[%112, %c0_i32_13] %119, %118, %true_8 {async_task_id = dense<1> : vector<1xi32>} : !tt.ptr<i8>, !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable> loc(#loc)
        %120 = ttg.memdesc_subview %arg33[%c0_i32_13, %c0_i32_13, %c0_i32_13] {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> loc(#loc179)
        %121 = ttg.memdesc_subview %arg34[%c0_i32_13, %c0_i32_13, %c0_i32_13] {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> loc(#loc182)
        ttng.tmem_store %cst_16, %121, %true_8 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x128xf32, #blocked> -> !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> loc(#loc182)
        %122:3 = scf.for %arg70 = %c0_i32_13 to %arg21 step %c128_i32_11 iter_args(%arg71 = %cst_18, %arg72 = %cst_17, %arg73 = %arg69) -> (tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, i64)  : i32 {
          %141 = arith.andi %arg73, %c1_i64_12 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc179)
          %142 = arith.trunci %141 {async_task_id = dense<1> : vector<1xi32>} : i64 to i1 loc(#loc179)
          %143 = ttg.memdesc_subview %arg35[%c0_i32_13] {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %144 = arith.extsi %142 {async_task_id = dense<1> : vector<1xi32>} : i1 to i32 loc(#loc179)
          ttng.wait_barrier %143, %144 {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc179)
          %result_20 = ttng.tmem_load %120 {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> -> tensor<64x128xf32, #blocked> loc(#loc179)
          %145 = ttg.memdesc_subview %arg60[%c0_i32_13] {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          ttng.mbarrier_arrive %145 {async_task_id = dense<1> : vector<1xi32>, operandSegmentSizes = array<i32: 1, 0, 0>, trackAsyncOp = false} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc179)
          // tt.print "softmax1: after softmax1 get qk1 " {hex = false, isSigned = array<i32: 2>} : %arg70 : i32
          %146 = "tt.reduce"(%result_20) <{axis = 1 : i32}> ({
          ^bb0(%arg74: f32 loc(callsite(#loc1 at #loc200)), %arg75: f32 loc(callsite(#loc1 at #loc200))):
            %171 = arith.maxnumf %arg74, %arg75 {async_task_id = dense<1> : vector<1xi32>} : f32 loc(#loc233)
            tt.reduce.return %171 {async_task_id = dense<1> : vector<1xi32>} : f32 loc(#loc227)
          }) {async_task_id = dense<1> : vector<1xi32>} : (tensor<64x128xf32, #blocked>) -> tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc227)
          %147 = arith.mulf %146, %91 {async_task_id = dense<1> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc201)
          %148 = arith.maxnumf %arg72, %147 {async_task_id = dense<1> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc202)
          %149 = arith.mulf %result_20, %92 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x128xf32, #blocked> loc(#loc203)
          %150 = tt.expand_dims %148 {async_task_id = dense<1> : vector<1xi32>, axis = 1 : i32} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64x1xf32, #blocked> loc(#loc204)
          %151 = tt.broadcast %150 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x1xf32, #blocked> -> tensor<64x128xf32, #blocked> loc(#loc205)
          %152 = arith.subf %149, %151 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x128xf32, #blocked> loc(#loc205)
          %153 = math.exp2 %152 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x128xf32, #blocked> loc(#loc206)
          %154 = "tt.reduce"(%153) <{axis = 1 : i32}> ({
          ^bb0(%arg74: f32 loc(callsite(#loc1 at #loc207)), %arg75: f32 loc(callsite(#loc1 at #loc207))):
            %171 = arith.addf %arg74, %arg75 {async_task_id = dense<1> : vector<1xi32>} : f32 loc(#loc234)
            tt.reduce.return %171 {async_task_id = dense<1> : vector<1xi32>} : f32 loc(#loc229)
          }) {async_task_id = dense<1> : vector<1xi32>} : (tensor<64x128xf32, #blocked>) -> tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc229)
          %155 = arith.subf %arg72, %148 {async_task_id = dense<1> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc208)
          %156 = math.exp2 %155 {async_task_id = dense<1> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc209)
          %157 = arith.mulf %arg71, %156 {async_task_id = dense<1> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc210)
          %158 = arith.addf %157, %154 {async_task_id = dense<1> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc211)
          %159 = tt.expand_dims %156 {async_task_id = dense<1> : vector<1xi32>, axis = 1 : i32} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64x1xf32, #blocked> loc(#loc212)
          %160 = tt.broadcast %159 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x1xf32, #blocked> -> tensor<64x128xf32, #blocked> loc(#loc213)
          %161 = ttg.memdesc_subview %arg36[%c0_i32_13] {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          // CHANGE from %144 to %168
          %167 = arith.xori %142, %true_8 {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc183)
          %168 = arith.extsi %167 {async_task_id = dense<1> : vector<1xi32>} : i1 to i32 loc(#loc183)
          ttng.wait_barrier %161, %168 {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc182)
          // tt.print "softmax1: before softmax1 get acc1 " {hex = false, isSigned = array<i32: 2>} : %arg70 : i32
          %result_21 = ttng.tmem_load %121 {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> -> tensor<64x128xf32, #blocked> loc(#loc182)
          %162 = ttg.memdesc_subview %arg64[%c0_i32_13] {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          ttng.mbarrier_arrive %162 {async_task_id = dense<1> : vector<1xi32>, operandSegmentSizes = array<i32: 1, 0, 0>, trackAsyncOp = false} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc182)
          %163 = arith.mulf %result_21, %160 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x128xf32, #blocked> loc(#loc213)
          %164 = arith.truncf %153 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x128xf32, #blocked> to tensor<64x128xbf16, #blocked> loc(#loc183)
          %165 = ttg.memdesc_subview %arg37[%c0_i32_13, %c0_i32_13, %c0_i32_13] {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable, 1x64x128> loc(#loc183)
          %166 = ttg.memdesc_subview %arg38[%c0_i32_13] {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          // %167 = arith.xori %142, %true_8 {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc183)
          // %168 = arith.extsi %167 {async_task_id = dense<1> : vector<1xi32>} : i1 to i32 loc(#loc183)
          ttng.wait_barrier %166, %168 {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc183)
          ttg.local_store %164, %165 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x128xbf16, #blocked> -> !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable, 1x64x128> loc(#loc183)
          %169 = ttg.memdesc_subview %arg65[%c0_i32_13] {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          ttng.mbarrier_arrive %169, %true_8 {async_task_id = dense<1> : vector<1xi32>, operandSegmentSizes = array<i32: 1, 1, 0>, trackAsyncOp = true} : !ttg.memdesc<1xi64, #shared, #smem, mutable>, i1 loc(#loc183)
          ttng.tmem_store %163, %121, %true_8 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x128xf32, #blocked> -> !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> loc(#loc182)
          %170 = arith.addi %arg73, %c1_i64_12 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc186)
          scf.yield {async_task_id = dense<1> : vector<1xi32>} %158, %148, %170 : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, i64 loc(#loc199)
        } {async_task_id = dense<1> : vector<1xi32>} loc(#loc186)
        %123 = math.log2 %122#0 {async_task_id = dense<1> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc165)
        %124 = arith.addf %122#1, %123 {async_task_id = dense<1> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc166)
        %125 = tt.expand_dims %122#0 {async_task_id = dense<1> : vector<1xi32>, axis = 1 : i32} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64x1xf32, #blocked> loc(#loc167)
        %126 = tt.broadcast %125 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x1xf32, #blocked> -> tensor<64x128xf32, #blocked> loc(#loc168)
        %result_19 = ttng.tmem_load %121 {async_task_id = dense<1> : vector<1xi32>} : !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> -> tensor<64x128xf32, #blocked> loc(#loc182)
        %127 = arith.divf %result_19, %126 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x128xf32, #blocked> loc(#loc168)
        %128 = arith.muli %96, %arg21 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc169)
        %129 = tt.addptr %arg39, %128 {async_task_id = dense<1> : vector<1xi32>} : !tt.ptr<f32>, i32 loc(#loc170)
        %130 = tt.splat %129 {async_task_id = dense<1> : vector<1xi32>} : !tt.ptr<f32> -> tensor<64x!tt.ptr<f32>, #blocked1> loc(#loc171)
        %131 = tt.addptr %130, %107 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x!tt.ptr<f32>, #blocked1>, tensor<64xi32, #blocked1> loc(#loc171)
        %132 = ttg.convert_layout %124 {async_task_id = dense<1> : vector<1xi32>} : tensor<64xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<64xf32, #blocked1> loc(#loc172)
        tt.store %131, %132 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x!tt.ptr<f32>, #blocked1> loc(#loc172)
        %133 = arith.divsi %103, %93 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc158)
        %134 = arith.addi %133, %109 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc173)
        %135 = arith.addi %134, %c64_i64 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc174)
        %136 = arith.trunci %135 {async_task_id = dense<1> : vector<1xi32>} : i64 to i32 loc(#loc175)
        %137 = arith.truncf %127 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x128xf32, #blocked> to tensor<64x128xbf16, #blocked> loc(#loc176)
        %138 = ttg.local_alloc %137 : (tensor<64x128xbf16, #blocked>) -> !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable> loc(#loc177)
        ttng.fence_async_shared {bCluster = false} loc(#loc177)
        ttng.async_tma_copy_local_to_global %85[%136, %c0_i32_13] %138 : !tt.ptr<i8>, !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable> loc(#loc177)
        ttng.async_tma_store_wait {pendings = 0 : i32} loc(#loc177)
        %139 = arith.addi %arg67, %75 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc77)
        %140 = arith.addi %arg68, %c1_i64_12 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc34)
        scf.yield {async_task_id = dense<1> : vector<1xi32>} %139, %140, %122#2 : i32, i64, i64 loc(#loc78)
      } {async_task_id = dense<1> : vector<1xi32>} loc(#loc34)
      ttg.warp_return {async_task_id = dense<1> : vector<1xi32>} loc(#loc)
    }
    partition1(%arg21: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg22: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg23: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg24: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg25: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg26: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg27: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg28: f32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg29: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg30: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg31: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg32: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg33: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc5 at #loc126)), %arg34: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc8 at #loc126)), %arg35: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg36: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg37: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(callsite(#loc9 at #loc126)), %arg38: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg39: !tt.ptr<f32> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg40: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg41: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg42: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc2 at #loc126)), %arg43: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg44: !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg45: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg46: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc6 at #loc126)), %arg47: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(callsite(#loc7 at #loc126)), %arg48: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg49: !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg50: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg51: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg52: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg53: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg54: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg55: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg56: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg57: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg58: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg59: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg60: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg61: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg62: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg63: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg64: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg65: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0)) num_warps(1) {
      %c2_i64 = arith.constant 2 : i64 loc(#loc1)
      %c1_i64_6 = arith.constant {async_task_id = dense<2> : vector<1xi32>} 1 : i64 loc(#loc1)
      %c0_i64_7 = arith.constant {async_task_id = dense<2> : vector<1xi32>} 0 : i64 loc(#loc1)
      %false = arith.constant {async_task_id = dense<2> : vector<1xi32>} false loc(#loc1)
      %true_8 = arith.constant {async_task_id = dense<2> : vector<1xi32>} true loc(#loc1)
      %c127_i32_9 = arith.constant {async_task_id = dense<2> : vector<1xi32>} 127 : i32 loc(#loc1)
      %c1_i32_10 = arith.constant {async_task_id = dense<2> : vector<1xi32>} 1 : i32 loc(#loc1)
      %c128_i32_11 = arith.constant {async_task_id = dense<2> : vector<1xi32>} 128 : i32 loc(#loc1)
      %c0_i32_12 = arith.constant {async_task_id = dense<2> : vector<1xi32>} 0 : i32 loc(#loc1)
      %72 = arith.addi %arg21, %c127_i32_9 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc127)
      %73 = arith.divsi %72, %c128_i32_11 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc128)
      %74 = tt.get_program_id x {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc13)
      %75 = tt.get_num_programs x {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc14)
      %76 = arith.muli %73, %arg22 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc15)
      %77 = arith.muli %76, %arg23 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc16)
      %78 = arith.divsi %77, %75 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc17)
      %79 = arith.remsi %77, %75 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc18)
      %80 = arith.cmpi slt, %74, %79 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc19)
      %81 = scf.if %80 -> (i32) {
        %83 = arith.addi %78, %c1_i32_10 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc21)
        scf.yield {async_task_id = dense<2> : vector<1xi32>} %83 : i32 loc(#loc21)
      } else {
        scf.yield {async_task_id = dense<2> : vector<1xi32>} %78 : i32 loc(#loc1)
      } {async_task_id = dense<2> : vector<1xi32>} loc(#loc20)
      %82:2 = scf.for %arg66 = %c0_i32_12 to %81 step %c1_i32_10 iter_args(%arg67 = %c0_i64_7, %arg68 = %c0_i64_7) -> (i64, i64)  : i32 {
        %83 = arith.andi %arg67, %c1_i64_6 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc144)
        %84 = arith.trunci %83 {async_task_id = dense<2> : vector<1xi32>} : i64 to i1 loc(#loc144)
        %85 = ttg.memdesc_subview %arg40[%c0_i32_12] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
        %86 = arith.extsi %84 {async_task_id = dense<2> : vector<1xi32>} : i1 to i32 loc(#loc)
        ttng.wait_barrier %85, %86 {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
        %87 = ttg.memdesc_subview %arg41[%c0_i32_12, %c0_i32_12, %c0_i32_12] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared1, #smem> loc(#loc)
        %88 = ttg.memdesc_subview %arg31[%c0_i32_12] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
        ttng.wait_barrier %88, %86 {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
        %89 = ttg.memdesc_subview %arg32[%c0_i32_12, %c0_i32_12, %c0_i32_12] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared1, #smem> loc(#loc)
        %90 = ttg.memdesc_subview %arg42[%c0_i32_12, %c0_i32_12, %c0_i32_12] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> loc(#loc178)
        %91 = ttg.memdesc_subview %arg33[%c0_i32_12, %c0_i32_12, %c0_i32_12] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> loc(#loc179)
        %92 = scf.for %arg69 = %c0_i32_12 to %arg21 step %c128_i32_11 iter_args(%arg70 = %arg68) -> (i64)  : i32 {
          %96 = arith.divui %arg70, %c2_i64 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc214)
          %97 = arith.muli %96, %c2_i64 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc214)
          %98 = arith.subi %arg70, %97 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc214)
          %99 = arith.trunci %98 {async_task_id = dense<2> : vector<1xi32>} : i64 to i32 loc(#loc214)
          %100 = arith.andi %96, %c1_i64_6 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc214)
          %101 = arith.trunci %100 {async_task_id = dense<2> : vector<1xi32>} : i64 to i1 loc(#loc214)
          %102 = ttg.memdesc_subview %arg43[%99] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %103 = arith.extsi %101 {async_task_id = dense<2> : vector<1xi32>} : i1 to i32 loc(#loc)
          // tt.print "dot0 role before first barrier " {hex = false, isSigned = array<i32: 2>} : %arg69 : i32
          ttng.wait_barrier %102, %103 {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          // tt.print "dot0 role after first barrier " {hex = false, isSigned = array<i32: 2>} : %arg69 : i32
          %104 = ttg.memdesc_subview %arg44[%99, %c0_i32_12, %c0_i32_12] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<128x128xbf16, #shared1, #smem> loc(#loc)
          %105 = ttg.memdesc_trans %104 {async_task_id = dense<2> : vector<1xi32>, order = array<i32: 1, 0>} : !ttg.memdesc<128x128xbf16, #shared1, #smem> -> !ttg.memdesc<128x128xbf16, #shared2, #smem> loc(#loc215)
          %106 = arith.andi %arg70, %c1_i64_6 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc178)
          %107 = arith.trunci %106 {async_task_id = dense<2> : vector<1xi32>} : i64 to i1 loc(#loc178)
          %108 = ttg.memdesc_subview %arg45[%c0_i32_12] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %109 = ttg.memdesc_subview %arg59[%c0_i32_12] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %110 = arith.xori %107, %true_8 : i1 loc(#loc178)
          %111 = arith.extui %110 : i1 to i32 loc(#loc178)
          // tt.print "dot0 role before second barrier " {hex = false, isSigned = array<i32: 2>} : %arg69 : i32
          ttng.wait_barrier %109, %111 {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc178)
          // tt.print "dot0 role after second barrier " {hex = false, isSigned = array<i32: 2>} : %arg69 : i32
          ttng.fence_async_shared {bCluster = false} loc(#loc178)
          // tt.print "dot0 role before dot0_s0 " {hex = false, isSigned = array<i32: 2>} : %arg70 : i64
          ttng.tc_gen5_mma %87, %105, %90, %false, %true_8, %108[%true_8] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<64x128xbf16, #shared1, #smem>, !ttg.memdesc<128x128xbf16, #shared2, #smem>, !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128>, !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc178)
          // tt.print "after dot0_s0 " {hex = false, isSigned = array<i32: 2>} : %arg70 : i64
          %112 = ttg.memdesc_subview %arg35[%c0_i32_12] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %113 = ttg.memdesc_subview %arg60[%c0_i32_12] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          ttng.wait_barrier %113, %111 {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc179)
          ttng.fence_async_shared {bCluster = false} loc(#loc179)
          ttng.tc_gen5_mma %89, %105, %91, %false, %true_8, %112[%true_8] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<64x128xbf16, #shared1, #smem>, !ttg.memdesc<128x128xbf16, #shared2, #smem>, !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128>, !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc179)
          // tt.print "after dot0_s1 " {hex = false, isSigned = array<i32: 2>} : %arg70 : i64
          %114 = ttg.memdesc_subview %arg58[%99] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          ttng.mbarrier_arrive %114 {async_task_id = dense<2> : vector<1xi32>, operandSegmentSizes = array<i32: 1, 0, 0>, trackAsyncOp = false} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc179)
          %115 = arith.addi %arg70, %c1_i64_6 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc186)
          scf.yield {async_task_id = dense<2> : vector<1xi32>} %115 : i64 loc(#loc199)
        } {async_task_id = dense<2> : vector<1xi32>} loc(#loc186)
        %93 = ttg.memdesc_subview %arg57[%c0_i32_12] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
        ttng.mbarrier_arrive %93 {async_task_id = dense<2> : vector<1xi32>, operandSegmentSizes = array<i32: 1, 0, 0>, trackAsyncOp = false} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc186)
        %94 = ttg.memdesc_subview %arg56[%c0_i32_12] {async_task_id = dense<2> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
        ttng.mbarrier_arrive %94 {async_task_id = dense<2> : vector<1xi32>, operandSegmentSizes = array<i32: 1, 0, 0>, trackAsyncOp = false} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc186)
        %95 = arith.addi %arg67, %c1_i64_6 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc34)
        scf.yield {async_task_id = dense<2> : vector<1xi32>} %95, %92 : i64, i64 loc(#loc78)
      } {async_task_id = dense<2> : vector<1xi32>} loc(#loc34)
      ttg.warp_return {async_task_id = dense<2> : vector<1xi32>} loc(#loc)
    }
    partition2(%arg21: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg22: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg23: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg24: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg25: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg26: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg27: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg28: f32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg29: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg30: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg31: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg32: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg33: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc5 at #loc126)), %arg34: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc8 at #loc126)), %arg35: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg36: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg37: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(callsite(#loc9 at #loc126)), %arg38: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg39: !tt.ptr<f32> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg40: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg41: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg42: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc2 at #loc126)), %arg43: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg44: !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg45: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg46: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc6 at #loc126)), %arg47: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(callsite(#loc7 at #loc126)), %arg48: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg49: !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg50: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg51: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg52: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg53: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg54: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg55: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg56: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg57: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg58: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg59: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg60: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg61: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg62: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg63: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg64: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg65: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0)) num_warps(1) {
      %c2_i64 = arith.constant 2 : i64 loc(#loc1)
      %c1_i64_6 = arith.constant {async_task_id = dense<3> : vector<1xi32>} 1 : i64 loc(#loc1)
      %c0_i64_7 = arith.constant {async_task_id = dense<3> : vector<1xi32>} 0 : i64 loc(#loc1)
      %true_8 = arith.constant {async_task_id = dense<3> : vector<1xi32>} true loc(#loc1)
      %c127_i32_9 = arith.constant {async_task_id = dense<3> : vector<1xi32>} 127 : i32 loc(#loc1)
      %c1_i32_10 = arith.constant {async_task_id = dense<3> : vector<1xi32>} 1 : i32 loc(#loc1)
      %c128_i32_11 = arith.constant {async_task_id = dense<3> : vector<1xi32>} 128 : i32 loc(#loc1)
      %c0_i32_12 = arith.constant {async_task_id = dense<3> : vector<1xi32>} 0 : i32 loc(#loc1)
      %72 = arith.addi %arg21, %c127_i32_9 {async_task_id = dense<3> : vector<1xi32>} : i32 loc(#loc127)
      %73 = arith.divsi %72, %c128_i32_11 {async_task_id = dense<3> : vector<1xi32>} : i32 loc(#loc128)
      %74 = tt.get_program_id x {async_task_id = dense<3> : vector<1xi32>} : i32 loc(#loc13)
      %75 = tt.get_num_programs x {async_task_id = dense<3> : vector<1xi32>} : i32 loc(#loc14)
      %76 = arith.muli %73, %arg22 {async_task_id = dense<3> : vector<1xi32>} : i32 loc(#loc15)
      %77 = arith.muli %76, %arg23 {async_task_id = dense<3> : vector<1xi32>} : i32 loc(#loc16)
      %78 = arith.divsi %77, %75 {async_task_id = dense<3> : vector<1xi32>} : i32 loc(#loc17)
      %79 = arith.remsi %77, %75 {async_task_id = dense<3> : vector<1xi32>} : i32 loc(#loc18)
      %80 = arith.cmpi slt, %74, %79 {async_task_id = dense<3> : vector<1xi32>} : i32 loc(#loc19)
      %81 = scf.if %80 -> (i32) {
        %83 = arith.addi %78, %c1_i32_10 {async_task_id = dense<3> : vector<1xi32>} : i32 loc(#loc21)
        scf.yield {async_task_id = dense<3> : vector<1xi32>} %83 : i32 loc(#loc21)
      } else {
        scf.yield {async_task_id = dense<3> : vector<1xi32>} %78 : i32 loc(#loc1)
      } {async_task_id = dense<3> : vector<1xi32>} loc(#loc20)
      %82 = scf.for %arg66 = %c0_i32_12 to %81 step %c1_i32_10 iter_args(%arg67 = %c0_i64_7) -> (i64)  : i32 {
        %83 = ttg.memdesc_subview %arg46[%c0_i32_12, %c0_i32_12, %c0_i32_12] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> loc(#loc180)
        %84 = ttg.memdesc_subview %arg34[%c0_i32_12, %c0_i32_12, %c0_i32_12] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128> loc(#loc182)
        %85 = scf.for %arg68 = %c0_i32_12 to %arg21 step %c128_i32_11 iter_args(%arg69 = %arg67) -> (i64)  : i32 {
          %86 = arith.andi %arg69, %c1_i64_6 {async_task_id = dense<3> : vector<1xi32>} : i64 loc(#loc180)
          %87 = arith.trunci %86 {async_task_id = dense<3> : vector<1xi32>} : i64 to i1 loc(#loc180)
          %88 = ttg.memdesc_subview %arg47[%c0_i32_12, %c0_i32_12, %c0_i32_12] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable, 1x64x128> loc(#loc181)
          %89 = ttg.memdesc_subview %arg37[%c0_i32_12, %c0_i32_12, %c0_i32_12] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable, 1x64x128> loc(#loc183)
          %90 = arith.divui %arg69, %c2_i64 {async_task_id = dense<3> : vector<1xi32>} : i64 loc(#loc216)
          %91 = arith.muli %90, %c2_i64 {async_task_id = dense<3> : vector<1xi32>} : i64 loc(#loc216)
          %92 = arith.subi %arg69, %91 {async_task_id = dense<3> : vector<1xi32>} : i64 loc(#loc216)
          %93 = arith.trunci %92 {async_task_id = dense<3> : vector<1xi32>} : i64 to i32 loc(#loc216)
          %94 = arith.andi %90, %c1_i64_6 {async_task_id = dense<3> : vector<1xi32>} : i64 loc(#loc216)
          %95 = arith.trunci %94 {async_task_id = dense<3> : vector<1xi32>} : i64 to i1 loc(#loc216)
          %96 = ttg.memdesc_subview %arg48[%93] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %97 = arith.extsi %95 {async_task_id = dense<3> : vector<1xi32>} : i1 to i32 loc(#loc)
          // tt.print "dot1 role before 1st barrier v " {hex = false, isSigned = array<i32: 2>} : %arg68 : i32
          ttng.wait_barrier %96, %97 {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          // tt.print "dot1 role after 1st barrier v " {hex = false, isSigned = array<i32: 2>} : %arg68 : i32
          %98 = ttg.memdesc_subview %arg49[%93, %c0_i32_12, %c0_i32_12] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<128x128xbf16, #shared1, #smem> loc(#loc)
          %99 = ttg.memdesc_subview %arg50[%c0_i32_12] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %100 = ttg.memdesc_subview %arg63[%c0_i32_12] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %101 = arith.extui %87 : i1 to i32 loc(#loc180)
          // tt.print "dot1 role before 2nd barrier p0 " {hex = false, isSigned = array<i32: 2>} : %arg68 : i32
          ttng.wait_barrier %100, %101 {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc180)
          // tt.print "dot1 role after 2nd barrier p0 " {hex = false, isSigned = array<i32: 2>} : %arg68 : i32
          %102 = ttg.memdesc_subview %arg51[%c0_i32_12] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %103 = ttg.memdesc_subview %arg62[%c0_i32_12] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %104 = arith.xori %87, %true_8 : i1 loc(#loc180)
          %105 = arith.extui %104 : i1 to i32 loc(#loc180)
          // tt.print "dot1 role before 3rd barrier acc0 phase " {hex = false, isSigned = array<i32: 2>} : %104 : i1
          // no need for producer acquire of acc0, we will be blocked by p0
          // ttng.wait_barrier %103, %105 {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc180)
          // tt.print "dot1 role before 3rd barrier acc0 " {hex = false, isSigned = array<i32: 2>} : %arg68 : i32
          ttng.fence_async_shared {bCluster = false} loc(#loc180)
          ttng.tc_gen5_mma %88, %98, %83, %true_8, %true_8, %99[%true_8], %102[%true_8] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable, 1x64x128>, !ttg.memdesc<128x128xbf16, #shared1, #smem>, !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc180)
          // tt.print "after dot1_s0 " {hex = false, isSigned = array<i32: 2>} : %arg69 : i64
          %106 = ttg.memdesc_subview %arg38[%c0_i32_12] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %107 = ttg.memdesc_subview %arg65[%c0_i32_12] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          ttng.wait_barrier %107, %101 {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc182)
          %108 = ttg.memdesc_subview %arg36[%c0_i32_12] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %109 = ttg.memdesc_subview %arg64[%c0_i32_12] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          // no need for producer acquire of acc1, we will be blocked by p1
          // ttng.wait_barrier %109, %105 {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc182)
          ttng.fence_async_shared {bCluster = false} loc(#loc182)
          ttng.tc_gen5_mma %89, %98, %84, %true_8, %true_8, %106[%true_8], %108[%true_8] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<64x128xbf16, #shared1, #smem, mutable, 1x64x128>, !ttg.memdesc<128x128xbf16, #shared1, #smem>, !ttg.memdesc<64x128xf32, #tmem, #ttng.tensor_memory, mutable, 1x64x128>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc182)
          // tt.print "after dot1_s1 " {hex = false, isSigned = array<i32: 2>} : %arg69 : i64
          %110 = ttg.memdesc_subview %arg61[%93] {async_task_id = dense<3> : vector<1xi32>} : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          ttng.mbarrier_arrive %110 {async_task_id = dense<3> : vector<1xi32>, operandSegmentSizes = array<i32: 1, 0, 0>, trackAsyncOp = false} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc182)
          %111 = arith.addi %arg69, %c1_i64_6 {async_task_id = dense<3> : vector<1xi32>} : i64 loc(#loc186)
          scf.yield {async_task_id = dense<3> : vector<1xi32>} %111 : i64 loc(#loc199)
        } {async_task_id = dense<3> : vector<1xi32>} loc(#loc186)
        scf.yield %85 : i64 loc(#loc78)
      } {async_task_id = dense<3> : vector<1xi32>} loc(#loc34)
      ttg.warp_return {async_task_id = dense<3> : vector<1xi32>} loc(#loc)
    }
    partition3(%arg21: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg22: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg23: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg24: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg25: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg26: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg27: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg28: f32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg29: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg30: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg31: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg32: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg33: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc5 at #loc126)), %arg34: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc8 at #loc126)), %arg35: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg36: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg37: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(callsite(#loc9 at #loc126)), %arg38: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg39: !tt.ptr<f32> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg40: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg41: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg42: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc2 at #loc126)), %arg43: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg44: !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg45: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg46: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc6 at #loc126)), %arg47: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(callsite(#loc7 at #loc126)), %arg48: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg49: !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg50: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg51: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg52: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg53: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg54: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg55: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg56: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg57: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg58: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg59: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg60: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg61: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg62: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg63: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg64: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg65: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0)) num_warps(1) {
      %c256_i64_6 = arith.constant 256 : i64 loc(#loc1)
      %c64_i32_7 = arith.constant 64 : i32 loc(#loc1)
      %c2_i64 = arith.constant 2 : i64 loc(#loc1)
      %true_8 = arith.constant true loc(#loc1)
      %c0_i64_9 = arith.constant {async_task_id = dense<4> : vector<1xi32>} 0 : i64 loc(#loc1)
      %c127_i32_10 = arith.constant {async_task_id = dense<4> : vector<1xi32>} 127 : i32 loc(#loc1)
      %c1_i32_11 = arith.constant {async_task_id = dense<4> : vector<1xi32>} 1 : i32 loc(#loc1)
      %c128_i32_12 = arith.constant {async_task_id = dense<4> : vector<1xi32>} 128 : i32 loc(#loc1)
      %c1_i64_13 = arith.constant {async_task_id = dense<4> : vector<1xi32>} 1 : i64 loc(#loc1)
      %c0_i32_14 = arith.constant {async_task_id = dense<4> : vector<1xi32>} 0 : i32 loc(#loc1)
      %72 = arith.addi %arg21, %c127_i32_10 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc127)
      %73 = arith.divsi %72, %c128_i32_12 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc128)
      %74 = tt.get_program_id x {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc13)
      %75 = tt.get_num_programs x {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc14)
      %76 = arith.muli %73, %arg22 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc15)
      %77 = arith.muli %76, %arg23 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc16)
      %78 = arith.divsi %77, %75 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc17)
      %79 = arith.remsi %77, %75 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc18)
      %80 = arith.cmpi slt, %74, %79 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc19)
      %81 = scf.if %80 -> (i32) {
        %89 = arith.addi %78, %c1_i32_11 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc21)
        scf.yield {async_task_id = dense<4> : vector<1xi32>} %89 : i32 loc(#loc21)
      } else {
        scf.yield {async_task_id = dense<4> : vector<1xi32>} %78 : i32 loc(#loc1)
      } {async_task_id = dense<4> : vector<1xi32>} loc(#loc20)
      %82 = arith.muli %arg22, %arg23 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc22)
      %83 = arith.muli %82, %arg21 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc23)
      %84 = ttg.global_scratch_alloc {alignment = 128 : i32, nbytes = 128 : i32} : !tt.ptr<i8> loc(#loc117)
      tt.experimental_tensormap_create %84, %arg52, [%c64_i32_7, %c128_i32_12], [%c128_i32_12, %83], [%c256_i64_6], [%c1_i32_11, %c1_i32_11] {elem_type = 10 : i32, fill_mode = 0 : i32, interleave_layout = 0 : i32, swizzle_mode = 3 : i32} : (!tt.ptr<i8>, !tt.ptr<bf16>, i32, i32, i32, i32, i64, i32, i32) -> () loc(#loc117)
      tt.experimental_tensormap_fenceproxy_acquire %84 : !tt.ptr<i8> loc(#loc117)
      %85 = arith.extsi %arg26 {async_task_id = dense<4> : vector<1xi32>} : i32 to i64 loc(#loc129)
      %86 = arith.extsi %arg27 {async_task_id = dense<4> : vector<1xi32>} : i32 to i64 loc(#loc130)
      %87 = arith.extsi %arg53 {async_task_id = dense<4> : vector<1xi32>} : i32 to i64 loc(#loc217)
      %88:2 = scf.for %arg66 = %c0_i32_14 to %81 step %c1_i32_11 iter_args(%arg67 = %74, %arg68 = %c0_i64_9) -> (i32, i64)  : i32 {
        %89 = arith.divsi %arg67, %73 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc36)
        %90 = arith.divsi %89, %arg23 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc135)
        %91 = arith.remsi %89, %arg23 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc136)
        %92 = arith.extsi %90 {async_task_id = dense<4> : vector<1xi32>} : i32 to i64 loc(#loc137)
        %93 = arith.muli %92, %85 {async_task_id = dense<4> : vector<1xi32>} : i64 loc(#loc129)
        %94 = arith.extsi %91 {async_task_id = dense<4> : vector<1xi32>} : i32 to i64 loc(#loc138)
        %95 = arith.muli %94, %86 {async_task_id = dense<4> : vector<1xi32>} : i64 loc(#loc130)
        %96 = arith.addi %93, %95 {async_task_id = dense<4> : vector<1xi32>} : i64 loc(#loc139)
        %97 = scf.for %arg69 = %c0_i32_14 to %arg21 step %c128_i32_12 iter_args(%arg70 = %arg68) -> (i64)  : i32 {
          %99 = arith.divsi %96, %87 {async_task_id = dense<4> : vector<1xi32>} : i64 loc(#loc217)
          %100 = arith.trunci %99 {async_task_id = dense<4> : vector<1xi32>} : i64 to i32 loc(#loc218)
          %101 = arith.addi %arg69, %100 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc219)
          %102 = arith.divui %arg70, %c2_i64 {async_task_id = dense<4> : vector<1xi32>} : i64 loc(#loc214)
          %103 = arith.muli %102, %c2_i64 {async_task_id = dense<4> : vector<1xi32>} : i64 loc(#loc214)
          %104 = arith.subi %arg70, %103 {async_task_id = dense<4> : vector<1xi32>} : i64 loc(#loc214)
          %105 = arith.trunci %104 {async_task_id = dense<4> : vector<1xi32>} : i64 to i32 loc(#loc214)
          %106 = arith.andi %102, %c1_i64_13 {async_task_id = dense<4> : vector<1xi32>} : i64 loc(#loc214)
          %107 = arith.trunci %106 {async_task_id = dense<4> : vector<1xi32>} : i64 to i1 loc(#loc214)
          %108 = ttg.memdesc_subview %arg58[%105] {async_task_id = dense<4> : vector<1xi32>} : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %109 = arith.xori %107, %true_8 : i1 loc(#loc214)
          %110 = arith.extui %109 : i1 to i32 loc(#loc214)
          ttng.wait_barrier %108, %110 {async_task_id = dense<4> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc214)
          %111 = ttg.memdesc_subview %arg43[%105] {async_task_id = dense<4> : vector<1xi32>} : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          ttng.barrier_expect %111, 32768 {async_task_id = dense<4> : vector<1xi32>}, %true_8 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %112 = ttg.memdesc_subview %arg44[%105, %c0_i32_14, %c0_i32_14] {async_task_id = dense<4> : vector<1xi32>} : !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<128x128xbf16, #shared1, #smem, mutable> loc(#loc)
          // tt.print "before tma k " {hex = false, isSigned = array<i32: 2>} : %arg69 : i32
          ttng.async_tma_copy_global_to_local %84[%101, %c0_i32_14] %112, %111, %true_8 {async_task_id = dense<4> : vector<1xi32>} : !tt.ptr<i8>, !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<128x128xbf16, #shared1, #smem, mutable> loc(#loc)
          %113 = arith.addi %arg70, %c1_i64_13 {async_task_id = dense<4> : vector<1xi32>} : i64 loc(#loc186)
          scf.yield {async_task_id = dense<4> : vector<1xi32>} %113 : i64 loc(#loc199)
        } {async_task_id = dense<4> : vector<1xi32>} loc(#loc186)
        %98 = arith.addi %arg67, %75 {async_task_id = dense<4> : vector<1xi32>} : i32 loc(#loc77)
        scf.yield %98, %97 : i32, i64 loc(#loc78)
      } {async_task_id = dense<4> : vector<1xi32>} loc(#loc34)
      ttg.warp_return {async_task_id = dense<4> : vector<1xi32>} loc(#loc)
    }
    partition4(%arg21: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg22: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg23: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg24: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg25: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg26: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg27: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg28: f32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg29: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg30: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg31: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg32: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg33: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc5 at #loc126)), %arg34: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc8 at #loc126)), %arg35: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg36: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg37: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(callsite(#loc9 at #loc126)), %arg38: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg39: !tt.ptr<f32> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg40: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg41: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg42: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc2 at #loc126)), %arg43: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg44: !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg45: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg46: !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(callsite(#loc6 at #loc126)), %arg47: !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable> loc(callsite(#loc7 at #loc126)), %arg48: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg49: !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg50: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg51: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg52: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg53: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg54: !tt.ptr<bf16> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg55: i32 loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg56: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg57: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg58: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg59: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg60: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg61: !ttg.memdesc<2xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg62: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg63: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg64: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0), %arg65: !ttg.memdesc<1xi64, #shared, #smem, mutable> loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1556:0)) num_warps(1) {
      %c256_i64_6 = arith.constant 256 : i64 loc(#loc1)
      %c64_i32_7 = arith.constant 64 : i32 loc(#loc1)
      %c2_i64 = arith.constant 2 : i64 loc(#loc1)
      %true_8 = arith.constant true loc(#loc1)
      %c0_i64_9 = arith.constant {async_task_id = dense<5> : vector<1xi32>} 0 : i64 loc(#loc1)
      %c127_i32_10 = arith.constant {async_task_id = dense<5> : vector<1xi32>} 127 : i32 loc(#loc1)
      %c1_i32_11 = arith.constant {async_task_id = dense<5> : vector<1xi32>} 1 : i32 loc(#loc1)
      %c128_i32_12 = arith.constant {async_task_id = dense<5> : vector<1xi32>} 128 : i32 loc(#loc1)
      %c1_i64_13 = arith.constant {async_task_id = dense<5> : vector<1xi32>} 1 : i64 loc(#loc1)
      %c0_i32_14 = arith.constant {async_task_id = dense<5> : vector<1xi32>} 0 : i32 loc(#loc1)
      %72 = arith.addi %arg21, %c127_i32_10 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc127)
      %73 = arith.divsi %72, %c128_i32_12 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc128)
      %74 = tt.get_program_id x {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc13)
      %75 = tt.get_num_programs x {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc14)
      %76 = arith.muli %73, %arg22 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc15)
      %77 = arith.muli %76, %arg23 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc16)
      %78 = arith.divsi %77, %75 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc17)
      %79 = arith.remsi %77, %75 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc18)
      %80 = arith.cmpi slt, %74, %79 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc19)
      %81 = scf.if %80 -> (i32) {
        %89 = arith.addi %78, %c1_i32_11 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc21)
        scf.yield {async_task_id = dense<5> : vector<1xi32>} %89 : i32 loc(#loc21)
      } else {
        scf.yield {async_task_id = dense<5> : vector<1xi32>} %78 : i32 loc(#loc1)
      } {async_task_id = dense<5> : vector<1xi32>} loc(#loc20)
      %82 = arith.muli %arg22, %arg23 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc22)
      %83 = arith.muli %82, %arg21 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc23)
      %84 = ttg.global_scratch_alloc {alignment = 128 : i32, nbytes = 128 : i32} : !tt.ptr<i8> loc(#loc121)
      tt.experimental_tensormap_create %84, %arg54, [%c64_i32_7, %c128_i32_12], [%c128_i32_12, %83], [%c256_i64_6], [%c1_i32_11, %c1_i32_11] {elem_type = 10 : i32, fill_mode = 0 : i32, interleave_layout = 0 : i32, swizzle_mode = 3 : i32} : (!tt.ptr<i8>, !tt.ptr<bf16>, i32, i32, i32, i32, i64, i32, i32) -> () loc(#loc121)
      tt.experimental_tensormap_fenceproxy_acquire %84 : !tt.ptr<i8> loc(#loc121)
      %85 = arith.extsi %arg26 {async_task_id = dense<5> : vector<1xi32>} : i32 to i64 loc(#loc129)
      %86 = arith.extsi %arg27 {async_task_id = dense<5> : vector<1xi32>} : i32 to i64 loc(#loc130)
      %87 = arith.extsi %arg55 {async_task_id = dense<5> : vector<1xi32>} : i32 to i64 loc(#loc220)
      %88:2 = scf.for %arg66 = %c0_i32_14 to %81 step %c1_i32_11 iter_args(%arg67 = %74, %arg68 = %c0_i64_9) -> (i32, i64)  : i32 {
        %89 = arith.divsi %arg67, %73 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc36)
        %90 = arith.divsi %89, %arg23 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc135)
        %91 = arith.remsi %89, %arg23 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc136)
        %92 = arith.extsi %90 {async_task_id = dense<5> : vector<1xi32>} : i32 to i64 loc(#loc137)
        %93 = arith.muli %92, %85 {async_task_id = dense<5> : vector<1xi32>} : i64 loc(#loc129)
        %94 = arith.extsi %91 {async_task_id = dense<5> : vector<1xi32>} : i32 to i64 loc(#loc138)
        %95 = arith.muli %94, %86 {async_task_id = dense<5> : vector<1xi32>} : i64 loc(#loc130)
        %96 = arith.addi %93, %95 {async_task_id = dense<5> : vector<1xi32>} : i64 loc(#loc139)
        %97 = scf.for %arg69 = %c0_i32_14 to %arg21 step %c128_i32_12 iter_args(%arg70 = %arg68) -> (i64)  : i32 {
          %99 = arith.divsi %96, %87 {async_task_id = dense<5> : vector<1xi32>} : i64 loc(#loc220)
          %100 = arith.extsi %arg69 {async_task_id = dense<5> : vector<1xi32>} : i32 to i64 loc(#loc221)
          %101 = arith.addi %99, %100 {async_task_id = dense<5> : vector<1xi32>} : i64 loc(#loc221)
          %102 = arith.trunci %101 {async_task_id = dense<5> : vector<1xi32>} : i64 to i32 loc(#loc222)
          %103 = arith.divui %arg70, %c2_i64 {async_task_id = dense<5> : vector<1xi32>} : i64 loc(#loc216)
          %104 = arith.muli %103, %c2_i64 {async_task_id = dense<5> : vector<1xi32>} : i64 loc(#loc216)
          %105 = arith.subi %arg70, %104 {async_task_id = dense<5> : vector<1xi32>} : i64 loc(#loc216)
          %106 = arith.trunci %105 {async_task_id = dense<5> : vector<1xi32>} : i64 to i32 loc(#loc216)
          %107 = arith.andi %103, %c1_i64_13 {async_task_id = dense<5> : vector<1xi32>} : i64 loc(#loc216)
          %108 = arith.trunci %107 {async_task_id = dense<5> : vector<1xi32>} : i64 to i1 loc(#loc216)
          %109 = ttg.memdesc_subview %arg61[%106] {async_task_id = dense<5> : vector<1xi32>} : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %110 = arith.xori %108, %true_8 : i1 loc(#loc216)
          %111 = arith.extui %110 : i1 to i32 loc(#loc216)
          ttng.wait_barrier %109, %111 {async_task_id = dense<5> : vector<1xi32>} : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc216)
          %112 = ttg.memdesc_subview %arg48[%106] {async_task_id = dense<5> : vector<1xi32>} : !ttg.memdesc<2xi64, #shared, #smem, mutable> -> !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          ttng.barrier_expect %112, 32768 {async_task_id = dense<5> : vector<1xi32>}, %true_8 : !ttg.memdesc<1xi64, #shared, #smem, mutable> loc(#loc)
          %113 = ttg.memdesc_subview %arg49[%106, %c0_i32_14, %c0_i32_14] {async_task_id = dense<5> : vector<1xi32>} : !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable> -> !ttg.memdesc<128x128xbf16, #shared1, #smem, mutable> loc(#loc)
          // tt.print "before tma v " {hex = false, isSigned = array<i32: 2>} : %arg69 : i32
          ttng.async_tma_copy_global_to_local %84[%102, %c0_i32_14] %113, %112, %true_8 {async_task_id = dense<5> : vector<1xi32>} : !tt.ptr<i8>, !ttg.memdesc<1xi64, #shared, #smem, mutable> -> !ttg.memdesc<128x128xbf16, #shared1, #smem, mutable> loc(#loc)
          %114 = arith.addi %arg70, %c1_i64_13 {async_task_id = dense<5> : vector<1xi32>} : i64 loc(#loc186)
          scf.yield {async_task_id = dense<5> : vector<1xi32>} %114 : i64 loc(#loc199)
        } {async_task_id = dense<5> : vector<1xi32>} loc(#loc186)
        %98 = arith.addi %arg67, %75 {async_task_id = dense<5> : vector<1xi32>} : i32 loc(#loc77)
        scf.yield %98, %97 : i32, i64 loc(#loc78)
      } {async_task_id = dense<5> : vector<1xi32>} loc(#loc34)
      ttg.warp_return {async_task_id = dense<5> : vector<1xi32>} loc(#loc)
    } : (i32, i32, i32, !tt.ptr<bf16>, !tt.ptr<bf16>, i32, i32, f32, i32, i32, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable>, !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !tt.ptr<f32>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable>, !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2xi64, #shared, #smem, mutable>, !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<1x64x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x64x128xbf16, #shared1, #smem, mutable>, !ttg.memdesc<2xi64, #shared, #smem, mutable>, !ttg.memdesc<2x128x128xbf16, #shared1, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !tt.ptr<bf16>, i32, !tt.ptr<bf16>, i32, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<2xi64, #shared, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<2xi64, #shared, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable>, !ttg.memdesc<1xi64, #shared, #smem, mutable>) -> () loc(#loc)
    tt.return loc(#loc125)
  } loc(#loc)
} loc(#loc)
#loc10 = loc("/home/mren/MetaMain3/triton/python/triton/language/standard.py":40:22)
#loc11 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1602:32)
#loc12 = loc("/home/mren/MetaMain3/triton/python/triton/language/standard.py":40:28)
#loc13 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1603:28)
#loc14 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1604:32)
#loc15 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1605:31)
#loc16 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1605:35)
#loc17 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1607:34)
#loc18 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1608:31)
#loc19 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1608:17)
#loc20 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1608:7)
#loc21 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1609:24)
#loc22 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1615:19)
#loc23 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1615:23)
#loc24 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1635:8)
#loc25 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1641:8)
#loc26 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":881:38)
#loc27 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":881:71)
#loc28 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":923:47)
#loc29 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":935:16)
#loc30 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":941:32)
#loc31 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":289:58)
#loc32 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":290:28)
#loc33 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1037:32)
#loc34 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1647:39)
#loc35 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1651:25)
#loc36 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1652:29)
#loc37 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":879:22)
#loc38 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":880:21)
#loc39 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":881:26)
#loc40 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":881:59)
#loc41 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":881:50)
#loc42 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":923:24)
#loc43 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":923:34)
#loc44 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":941:44)
#loc45 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":941:66)
#loc46 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":941:16)
#loc47 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":266:36)
#loc48 = loc("/home/mren/MetaMain3/triton/python/triton/language/standard.py":188:40)
#loc50 = loc("/home/mren/MetaMain3/triton/python/triton/language/standard.py":167:27)
#loc51 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":289:41)
#loc52 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":290:45)
#loc53 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":290:39)
#loc54 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":291:30)
#loc55 = loc("/home/mren/MetaMain3/triton/python/triton/language/standard.py":290:36)
#loc57 = loc("/home/mren/MetaMain3/triton/python/triton/language/standard.py":260:15)
#loc58 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":294:41)
#loc59 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":294:34)
#loc60 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":295:26)
#loc61 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":295:35)
#loc62 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":297:33)
#loc63 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":297:26)
#loc64 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":346:8)
#loc65 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1031:29)
#loc66 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1031:16)
#loc67 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1032:27)
#loc68 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1032:22)
#loc69 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1033:31)
#loc70 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1033:22)
#loc71 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1033:39)
#loc72 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1034:26)
#loc73 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1037:44)
#loc74 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1037:66)
#loc75 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1038:24)
#loc76 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1038:16)
#loc77 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1699:20)
#loc78 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1699:8)
#loc79 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":948:32)
#loc80 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1049:32)
#loc81 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":924:34)
#loc82 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":924:49)
#loc83 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":948:44)
#loc84 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":948:64)
#loc85 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":948:81)
#loc86 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":948:16)
#loc88 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":312:58)
#loc89 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":312:41)
#loc90 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":313:28)
#loc91 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":313:45)
#loc92 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":313:39)
#loc93 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":314:30)
#loc95 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":317:41)
#loc96 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":317:34)
#loc97 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":318:26)
#loc98 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":318:35)
#loc99 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":320:33)
#loc100 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":320:26)
#loc101 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1043:29)
#loc102 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1043:16)
#loc103 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1044:27)
#loc104 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1044:22)
#loc105 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1045:31)
#loc106 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1045:22)
#loc107 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1045:39)
#loc108 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1046:26)
#loc109 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1049:44)
#loc110 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1049:64)
#loc111 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1049:81)
#loc112 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1050:24)
#loc113 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1050:16)
#loc114 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":272:20)
#loc115 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":279:29)
#loc116 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":336:24)
#loc117 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1614:8)
#loc118 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":272:59)
#loc119 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":272:73)
#loc120 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":272:44)
#loc121 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1628:12)
#loc122 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":336:40)
#loc123 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":336:52)
#loc124 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":336:64)
#loc125 = loc("/home/mren/OpenSource2/tritonbench/tritonbench/kernels/triton_fused_attention.py":1647:4)
#loc127 = loc(callsite(#loc10 at #loc11))
#loc128 = loc(callsite(#loc12 at #loc11))
#loc129 = loc(callsite(#loc26 at #loc4))
#loc130 = loc(callsite(#loc27 at #loc4))
#loc131 = loc(callsite(#loc28 at #loc4))
#loc132 = loc(callsite(#loc29 at #loc4))
#loc133 = loc(callsite(#loc30 at #loc4))
#loc134 = loc(callsite(#loc33 at #loc4))
#loc135 = loc(callsite(#loc37 at #loc4))
#loc136 = loc(callsite(#loc38 at #loc4))
#loc137 = loc(callsite(#loc39 at #loc4))
#loc138 = loc(callsite(#loc40 at #loc4))
#loc139 = loc(callsite(#loc41 at #loc4))
#loc140 = loc(callsite(#loc42 at #loc4))
#loc141 = loc(callsite(#loc43 at #loc4))
#loc142 = loc(callsite(#loc44 at #loc4))
#loc143 = loc(callsite(#loc45 at #loc4))
#loc144 = loc(callsite(#loc46 at #loc4))
#loc145 = loc(callsite(#loc65 at #loc4))
#loc146 = loc(callsite(#loc66 at #loc4))
#loc147 = loc(callsite(#loc67 at #loc4))
#loc148 = loc(callsite(#loc68 at #loc4))
#loc149 = loc(callsite(#loc69 at #loc4))
#loc150 = loc(callsite(#loc70 at #loc4))
#loc151 = loc(callsite(#loc71 at #loc4))
#loc152 = loc(callsite(#loc72 at #loc4))
#loc153 = loc(callsite(#loc73 at #loc4))
#loc154 = loc(callsite(#loc74 at #loc4))
#loc155 = loc(callsite(#loc75 at #loc4))
#loc156 = loc(callsite(#loc76 at #loc4))
#loc157 = loc(callsite(#loc79 at #loc4))
#loc158 = loc(callsite(#loc80 at #loc4))
#loc159 = loc(callsite(#loc81 at #loc4))
#loc160 = loc(callsite(#loc82 at #loc4))
#loc161 = loc(callsite(#loc83 at #loc4))
#loc162 = loc(callsite(#loc84 at #loc4))
#loc163 = loc(callsite(#loc85 at #loc4))
#loc164 = loc(callsite(#loc86 at #loc4))
#loc165 = loc(callsite(#loc101 at #loc4))
#loc166 = loc(callsite(#loc102 at #loc4))
#loc167 = loc(callsite(#loc103 at #loc4))
#loc168 = loc(callsite(#loc104 at #loc4))
#loc169 = loc(callsite(#loc105 at #loc4))
#loc170 = loc(callsite(#loc106 at #loc4))
#loc171 = loc(callsite(#loc107 at #loc4))
#loc172 = loc(callsite(#loc108 at #loc4))
#loc173 = loc(callsite(#loc109 at #loc4))
#loc174 = loc(callsite(#loc110 at #loc4))
#loc175 = loc(callsite(#loc111 at #loc4))
#loc176 = loc(callsite(#loc112 at #loc4))
#loc177 = loc(callsite(#loc113 at #loc4))
#loc184 = loc(callsite(#loc31 at #loc126))
#loc185 = loc(callsite(#loc32 at #loc126))
#loc186 = loc(callsite(#loc47 at #loc126))
#loc188 = loc(callsite(#loc51 at #loc126))
#loc189 = loc(callsite(#loc52 at #loc126))
#loc190 = loc(callsite(#loc53 at #loc126))
#loc191 = loc(callsite(#loc54 at #loc126))
#loc193 = loc(callsite(#loc58 at #loc126))
#loc194 = loc(callsite(#loc59 at #loc126))
#loc195 = loc(callsite(#loc60 at #loc126))
#loc196 = loc(callsite(#loc61 at #loc126))
#loc197 = loc(callsite(#loc62 at #loc126))
#loc198 = loc(callsite(#loc63 at #loc126))
#loc199 = loc(callsite(#loc64 at #loc126))
#loc201 = loc(callsite(#loc88 at #loc126))
#loc202 = loc(callsite(#loc89 at #loc126))
#loc203 = loc(callsite(#loc90 at #loc126))
#loc204 = loc(callsite(#loc91 at #loc126))
#loc205 = loc(callsite(#loc92 at #loc126))
#loc206 = loc(callsite(#loc93 at #loc126))
#loc208 = loc(callsite(#loc95 at #loc126))
#loc209 = loc(callsite(#loc96 at #loc126))
#loc210 = loc(callsite(#loc97 at #loc126))
#loc211 = loc(callsite(#loc98 at #loc126))
#loc212 = loc(callsite(#loc99 at #loc126))
#loc213 = loc(callsite(#loc100 at #loc126))
#loc214 = loc(callsite(#loc114 at #loc126))
#loc215 = loc(callsite(#loc115 at #loc126))
#loc216 = loc(callsite(#loc116 at #loc126))
#loc217 = loc(callsite(#loc118 at #loc126))
#loc218 = loc(callsite(#loc119 at #loc126))
#loc219 = loc(callsite(#loc120 at #loc126))
#loc220 = loc(callsite(#loc122 at #loc126))
#loc221 = loc(callsite(#loc123 at #loc126))
#loc222 = loc(callsite(#loc124 at #loc126))
#loc223 = loc(callsite(#loc48 at #loc187))
#loc225 = loc(callsite(#loc55 at #loc192))
#loc227 = loc(callsite(#loc48 at #loc200))
#loc229 = loc(callsite(#loc55 at #loc207))
#loc231 = loc(callsite(#loc50 at #loc223))
#loc232 = loc(callsite(#loc57 at #loc225))
#loc233 = loc(callsite(#loc50 at #loc227))
#loc234 = loc(callsite(#loc57 at #loc229))
